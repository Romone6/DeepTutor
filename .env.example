# ============================================================================
# Deep-Tutor Environment Configuration
# Copy this file to .env and fill in your API keys and settings.
# All API keys should be kept confidential and never committed to version control.
# You can adjust the max_token parameters in DeepTutor\config\agents.yaml.
# ============================================================================

# ============================================================================
# Server Configuration
# ============================================================================
# Configure the server ports and API URL for remote access.

# Backend API port (default: 8001)
# BACKEND_PORT=8001

# Frontend web port (default: 3782)
# FRONTEND_PORT=3782

# Frontend API Base URL (for remote/LAN access)
# Set this when accessing DeepTutor from another device on your network.
# Example: If your server IP is 192.168.1.100, set:
#   NEXT_PUBLIC_API_BASE=http://192.168.1.100:8001
# If not set, defaults to http://localhost:8001 (only works on the local machine)

# NEXT_PUBLIC_API_BASE=http://your-server-ip:8001 (optional)

# ============================================================================
# LLM (Large Language Model) Configuration
# ============================================================================
# Configure the main AI model used for reasoning, generation, and conversation.
# Supports OpenAI-compatible APIs (OpenAI, Azure OpenAI, DeepSeek, Qwen, Ollama, etc.)

# LLM service provider type
# Options: openai, azure_openai, ollama, lollms
LLM_BINDING=openai

# Model name for the LLM
# For OpenAI: gpt-4, gpt-4-turbo, gpt-3.5-turbo
# For Ollama: llama3.1:8b-instruct, qwen2.5:7b-instruct, llama3.2:3b
# For vLLM: llama-3.1-8b-instruct, qwen2.5-7b-instruct
LLM_MODEL=

# LLM API endpoint URL
# For OpenAI: https://api.openai.com/v1
# For Azure: https://{resource}.openai.azure.com/{deployment}/v1
# For Ollama: http://localhost:11434 (or your Ollama server IP)
# For vLLM: http://localhost:8000 (vLLM OpenAI-compatible endpoint)
LLM_HOST=

# LLM API authentication key
# For OpenAI: sk-...
# For Azure: your Azure API key
# For Ollama/vLLM: leave empty (no auth required by default)
LLM_API_KEY=

# ============================================================================
# Ollama-Specific Configuration (when LLM_BINDING=ollama)
# ============================================================================
# Ollama base URL (can be different from LLM_HOST for embedded scenarios)
# OLLAMA_BASE_URL=http://localhost:11434

# Model to use with Ollama (overrides LLM_MODEL when using Ollama)
# OLLAMA_MODEL=llama3.1:8b-instruct

# ============================================================================
# vLLM Runtime Configuration (alternative high-performance option)
# ============================================================================
# vLLM provides higher throughput for local inference with GPU acceleration.
# Use when you have a capable GPU (RTX 4070 Super or better recommended).

# Runtime selection: choose between "ollama" or "vllm"
# LLM_RUNTIME=vllm

# vLLM server endpoint (default: http://localhost:8000/v1)
# VLLM_BASE_URL=http://localhost:8000/v1

# Model name for vLLM (use HuggingFace model IDs or local model path)
# Recommended for RTX 4070 Super:
#   - meta-llama/Llama-3.1-8B-Instruct (8GB VRAM)
#   - Qwen/Qwen2.5-7B-Instruct (7GB VRAM)
#   - meta-llama/Llama-3.2-3B-Instruct (3GB VRAM, faster)
# VLLM_MODEL=meta-llama/Llama-3.1-8B-Instruct

# vLLM-specific settings (optional)
# VLLM_TENSOR_PARALLEL_SIZE=1  # Set to GPU count for multi-GPU
# VLLM_GPU_MEMORY_UTILIZATION=0.9  # GPU memory allocation ratio
# VLLM_MAX_MODEL_LEN=8192  # Context window size

# ============================================================================
# Embedding Model Configuration
# ============================================================================
# Configure the text embedding model used for semantic search and RAG.
# Required for knowledge base functionality.

# Embedding service provider type
# Options: openai, azure_openai, ollama, lollms
EMBEDDING_BINDING=openai

# Embedding model name
# Examples: text-embedding-3-large, text-embedding-3-small, text-embedding-ada-002
# For Ollama: nomic-embed-text, mxbai-embed-large
EMBEDDING_MODEL=text-embedding-3-large

# Embedding vector dimension
EMBEDDING_DIMENSION=3072

# Embedding API endpoint URL
EMBEDDING_HOST=

# Embedding API authentication key
EMBEDDING_API_KEY=

# ============================================================================
# TTS (Text-to-Speech) Configuration (Optional)
# ============================================================================
# Configure voice synthesis for the Co-Writer(Interactive IdeaGen) narration feature.
# Also remember to choose a voice in DeepTutor\config\main.yaml.

# TTS model name
TTS_MODEL=

# TTS API endpoint URL
TTS_URL=

# TTS API authentication key
TTS_API_KEY=

# ============================================================================
# Web Search Configuration
# ============================================================================
# Configure external search APIs for research features.

# Search provider to use for web search
# Options: perplexity, baidu
# Default: perplexity
SEARCH_PROVIDER=perplexity

# ----------------------------------------------------------------------------
# Perplexity AI Search Configuration
# ----------------------------------------------------------------------------
# Perplexity API key for web search functionality
# Get your API key at: https://www.perplexity.ai/settings/api
PERPLEXITY_API_KEY=

# ----------------------------------------------------------------------------
# Baidu AI Search Configuration (鐧惧害AI鎼滅储)
# ----------------------------------------------------------------------------
# Baidu API Key for intelligent search and generation
# Get your API key at: https://console.bce.baidu.com/ai_apaas/resource
# Format: bce-v3/ALTAK-xxx/xxx or the full API key
BAIDU_API_KEY=

# ============================================================================
# Logging Configuration
# ============================================================================

# Log level for RAG tool module
# Options: DEBUG, INFO, WARNING, ERROR
RAG_TOOL_MODULE_LOG_LEVEL=INFO
